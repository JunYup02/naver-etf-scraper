# run_weekly_analysis.py
import os
import time
import pandas as pd
import glob
from datetime import datetime
from tqdm import tqdm
from src import scraper, processor, db

def _load_latest_krx_daily_snapshot():
    """ìµœì‹  KRX ë°ì´í„° ë¡œë“œ (ì¢…ëª© ë¦¬ìŠ¤íŠ¸ í™•ë³´ìš©)"""
    pattern = "data/krx_daily/krx_data_*.csv"
    list_of_files = glob.glob(pattern)
    
    if not list_of_files:
        raise FileNotFoundError("KRX ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. run_daily_krx.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    latest_file = max(list_of_files, key=os.path.basename)
    print(f"[INFO] KRX ê¸°ì¤€ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë“œ: {latest_file}")

    df = pd.read_csv(latest_file, encoding='utf-8-sig')
    if 'ë‹¨ì¶•ì½”ë“œ' in df.columns:
        df['ë‹¨ì¶•ì½”ë“œ'] = df['ë‹¨ì¶•ì½”ë“œ'].astype(str).str.zfill(6)
    
    return df[['ë‹¨ì¶•ì½”ë“œ', 'í•œê¸€ì¢…ëª©ëª…']]

def run():
    print("=== ğŸ“Š ì£¼ê°„ ETF ìƒì„¸ ë¶„ì„ (ë¶„í•´ ë°ì´í„° ì ì¬) ===")

    # 1. ëŒ€ìƒ ì¢…ëª© ë¡œë“œ
    try:
        krx_daily_df = _load_latest_krx_daily_snapshot()
        tickers = krx_daily_df["ë‹¨ì¶•ì½”ë“œ"].tolist()
        print(f"[INFO] ìˆ˜ì§‘ ëŒ€ìƒ: ì´ {len(tickers)}ê°œ ì¢…ëª©")
    except Exception as e:
        print(f"[FATAL] ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    results = []

    # 2. ë„¤ì´ë²„ í¬ë¡¤ë§
    print(f"[INFO] ë„¤ì´ë²„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    for code in tqdm(tickers, desc="Processing ETFs", unit="ì¢…ëª©"):
        try:
            time.sleep(0.3) # ì„œë²„ ë¶€í•˜ ë°©ì§€
            basic = scraper.fetch_etf_basic(code)
            analysis = scraper.fetch_etf_analysis(code)

            if not basic or not analysis:
                continue

            merged = {**basic, **analysis}
            results.append(merged)

        except Exception as e:
            continue

    if results:
        df = pd.DataFrame(results)

        # -----------------------------------------------------------
        # [STEP 1] ê¸°ë³¸ ì»¬ëŸ¼ ë§¤í•‘ (DB ì»¬ëŸ¼ëª… ê¸°ì¤€)
        # -----------------------------------------------------------
        rename_map = {
            'code': 'ticker', 
            'name': 'name', 
            'nav': 'nav', 
            'price': 'price', 
            'market_cap': 'market_cap', 
            'inflow_1m': 'inflow_1m',
            'fee': 'fee', 
            'distribution_yield': 'distribution_yield', 
            'tracking_error': 'tracking_error',
            'return_1m': 'return_1m', 
            'return_6m': 'return_6m', 
            'return_1y': 'return_1y',
            'top_holdings': 'top_holdings', 
            'sector_weight': 'sector_weight',    # ì›ë³¸(Text)ë„ ìœ ì§€
            'country_weight': 'country_weight',  # ì›ë³¸(Text)ë„ ìœ ì§€
            'issuer': 'issuer',
            'listed_date': 'listed_date'
        }
        df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

        # -----------------------------------------------------------
        # [STEP 2] ì „ì²˜ë¦¬ (ìˆ«ì ë³€í™˜ + ì»¬ëŸ¼ ìª¼ê°œê¸°)
        # -----------------------------------------------------------
        # ì—¬ê¸°ì„œ sector_1, sector_1_pct ë“±ì´ DataFrameì— ìƒì„±ë©ë‹ˆë‹¤.
        print("\n[PROC] ì „ì²˜ë¦¬(ìˆ«ì ë³€í™˜ ë° ë¹„ì¤‘ ë¶„í•´) ìˆ˜í–‰ ì¤‘...")
        df = processor.preprocess_etf_data(df)
        
        # ê¸°ì¤€ì¼ ì¶”ê°€
        df['std_date'] = datetime.now().date()

        # -----------------------------------------------------------
        # [STEP 3] CSV ì €ì¥ (ëª¨ë“  ë°ì´í„° í¬í•¨)
        # -----------------------------------------------------------
        today_str = datetime.now().strftime("%Y%m%d")
        os.makedirs("data/output", exist_ok=True)
        csv_path = f"data/output/etf_weekly_analysis_report_{today_str}.csv"
        
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"[SAVE] CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

        # -----------------------------------------------------------
        # [STEP 4] DB ì ì¬ (â˜… Top 3 ì»¬ëŸ¼ í¬í•¨ â˜…)
        # -----------------------------------------------------------
        print("[DB] Cloud SQL ì ì¬ ì‹œì‘...")

        # 1. ê¸°ë³¸ ì•½ì†ëœ ì»¬ëŸ¼ë“¤
        base_cols = list(rename_map.values()) + ['std_date']
        
        # 2. ìª¼ê°œì§„ ì»¬ëŸ¼ë“¤ ì¤‘ DBì— ë„£ì„ ê²ƒë“¤ (Top 1~3)
        # DB í…Œì´ë¸”ì— sector_4 ì´ìƒì€ ì—†ìœ¼ë¯€ë¡œ, ë”± 3ê°œê¹Œì§€ë§Œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŠµë‹ˆë‹¤.
        split_cols = []
        for i in range(1, 4): # 1, 2, 3
            split_cols.extend([
                f'sector_{i}', f'sector_{i}_pct',
                f'country_{i}', f'country_{i}_pct'
            ])
            
        # 3. ìµœì¢… DBìš© ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        valid_db_cols = base_cols + split_cols
        
        # 4. ë°ì´í„°í”„ë ˆì„ í•„í„°ë§ (DBì— ìˆëŠ” ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦¼)
        final_db_df = df[[c for c in valid_db_cols if c in df.columns]].copy()
        
        # ë‚ ì§œ íƒ€ì… ë³´ì •
        if 'listed_date' in final_db_df.columns:
            final_db_df['listed_date'] = pd.to_datetime(final_db_df['listed_date'], errors='coerce')

        db.insert_dataframe(final_db_df, 'etf_analysis')

    else:
        print("[WARN] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run()